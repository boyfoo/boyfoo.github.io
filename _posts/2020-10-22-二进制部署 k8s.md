---
layout: post
title: '二进制部署 k8s'
date: 2020-10-22
author: boyfoo
tags: k8s
---

### 一、准备工作

#### 1.软件信息

* Docker version 18.06.3-ce
* CentOS 7.6.1811
* K8s v1.16.0

两台节点：

1. 192.168.205.120 node01 (master节点，worker节点)
2. 192.168.205.121 node02 (worker节点)

#### 2.关闭防火墙
<!-- tar xvf TLS.tar.gz -->
* centos

```bash
$ systemctl stop firewalld
$ systemctl disable firewalld
```

* ubuntu

`TODO`

#### 3.关闭交换分区

* centos

```bash
# 临时关闭
$ swapoff -a
```
```bash
# 永久关闭 注释文件内容
$ vim /etc/fstab
#/swapfile none swap defaults 0 0
```
```bash
#查看关闭结果
$ free -m
# Swap 都为0表示关闭成功
Swap:             0           0           0
```

* ubuntu

`TODO`

#### 4.配置主机名称名称解析

```bash
$ vim /etc/hosts

192.168.205.120 node01
192.168.205.121 node02
```


#### 5.关闭 setlinux

* centos

```bash
# 临时
$ setenforce 0

# 永久
$ vim /etc/selinux/config
SELINUX=disabled
```

* ubuntu

`TODO`

#### 6.配置时间

以主节点时间为准，副节点获取主节点的时间为准

* centos

主节点:

```bash
$ yum install chrony -y

$ vim /etc/chrony.conf
# 以本地时间地址 注释其他server
server 127.127.1.0 iburst
# 允许该网段的副接口来请求 集群的网段
allow 192.168.205.0/24
local stratum 10

$ systemctl restart chronyd
$ systemctl enable chronyd

# 查看进程端口是否被监听
$ ss -unl | grep 123
```

副节点:

```bash
$ yum install chrony -y

$ vim /etc/chrony.conf
# 以本地时间地址 注释其他server
server [主节点IP] iburst

$ systemctl restart chronyd
$ systemctl enable chronyd

# 查看结果 ^*开头表示成功
$ chronyc sources
^* node01                       10   6    17    41  +8305ns[  +54us] +/-  160us
```

* ubuntu
`TODO`

#### 7.桥接IPV4流量传递到iptables的链

* centos

```bash
$ cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# 刷新生效
$ sudo sysctl --system
```
* ubuntu
`TODO`

### 二、自签证书

#### 1.自建CA

方式有2种：
* openssl
* cfssl

`k8s` 官方推荐使用 `cfssl`

下载cfssl工具组件

```bash
$ curl -s -L -o /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
$ curl -s -L -o /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
$ curl -s -L -o /bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64

# 赋予运行权限
$ chmod +x /bin/cfssl*
```
#### 2.颁发ECTD证书

文件统一生成在`/opt/cfssl_file/`下

创建证书颁发机构：

`$ vim etcd/ca-csr.json`

```json
{
    "CN": "etcd CA",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing"
        }
    ]
}
```

`$ vim etcd/ca-config.json` 

```json
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "www": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
```

填写颁发表单：

`$ vim etcd/server-csr.json`

```json
{
    "CN": "etcd",
    "hosts": [
        "192.168.205.120",
        "192.168.205.121"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing"
        }
    ]
}
```

颁发证书:

```bash
$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server

# 查看生成的证书文件 
$ ls *pem
```

安装`etcd`:

目前演示只在主机安装`etcd`，其他节点上只用`etcdcli`客户端，统一访问主节点的`etcd`，主节点生成证书启动后，证书要复制到其他节点上，供`etcdcli`客户端使用

```bash
$ yum install -y etcd-3.2.28-1.el7_8

# 查看系统脚本 修改ExecStart部分 和 证书配置文件
$ vim /usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=/etc/etcd/etcd.conf
ExecStart=/usr/bin/etcd \
        --name=${ETCD_NAME} \
        --data-dir=${ETCD_DATA_DIR} \
        --listen-peer-urls=${ETCD_LISTEN_PEER_URLS} \
        --listen-client-urls=${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \
        --advertise-client-urls=${ETCD_ADVERTISE_CLIENT_URLS} \
        --initial-advertise-peer-urls=${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
        --initial-cluster=${ETCD_INITIAL_CLUSTER} \
        --initial-cluster-token=${ETCD_INITIAL_CLUSTER_TOKEN} \
        --initial-cluster-state=new \
        --cert-file=/opt/cfssl_file/etcd/server.pem \
        --key-file=/opt/cfssl_file/etcd/server-key.pem \
        --peer-cert-file=/opt/cfssl_file/etcd/server.pem \
        --peer-key-file=/opt/cfssl_file/etcd/server-key.pem \
        --trusted-ca-file=/opt/cfssl_file/etcd/ca.pem \
        --peer-trusted-ca-file=/opt/cfssl_file/etcd/ca.pem
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

修改配置文件

```bash
$ vim /etc/etcd/etcd.conf
ETCD_LISTEN_PEER_URLS="https://192.168.205.120:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.205.120:2379"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.205.120:2379"
ETCD_NAME="node01" # 节点名称
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.205.120:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.205.120:2379"
ETCD_INITIAL_CLUSTER="node01=https://localhost:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"

systemctl daemon-reload
systemctl restart etcd.service
systemctl enable etcd.service
```

其他节点 `etcdcli` 客户端访问

先拷贝证书到节点上

```bash
$ scp -r /opt/cfssl_file/etcd/ root@node02:/opt/cfssl_file/etcd/
```

```bash
# 其他节点访问主机etcd
$ etcdctl --ca-file=/opt/cfssl_file/etcd/ca.pem --cert-file=/opt/cfssl_file/etcd/server.pem --key-file=/opt/cfssl_file/etcd/server-key.pem --endpoints=https://192.168.205.120:2379 get zx

# 查看集群状态 (若失败要带上秘钥)
$ etcdctl --endpoints=https://192.168.205.120:2379 cluster-health
```

#### 3.颁发apiserver证书

主节点颁布证书:

```bash
$ vim k8s/ca-csr.json
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
      	    "O": "k8s",
            "OU": "System"
        }
    ]
}

$ vim k8s/ca-config.json
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
# k8s apiserver证书配置
$ vim k8s/server-csr.json
{
    "CN": "kubernetes",
    "hosts": [
      "10.0.0.1",
      "127.0.0.1",
      "kubernetes",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local",
      "192.168.205.121",
      "192.168.205.120"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
# k8s代理证书配置
$ vim kube-proxy-csr.json
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}

# 生成
$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server
$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
```

自行下载`k8s`

配置`apiserver`：

```bash
$ vim /usr/lib/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-apiserver.conf
ExecStart=/opt/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target

$ vim /opt/kubernetes/cfg/kube-apiserver.conf
KUBE_APISERVER_OPTS="--logtostderr=false \  // 错误信息是否打印到控制台
--v=2 \ // 日志级别
--log-dir=/opt/kubernetes/logs \    // 日志文件位置
--etcd-servers=https://192.168.205.120:2379 \	//etcd地址(多个逗号隔开)
--bind-address=192.168.205.120 \	// 当前主机地址
--secure-port=6443 \
--advertise-address=192.168.205.120 \	// 通告地址 告诉别人来访问这个 正常与主机地址一样
--allow-privileged=true \   // 是否已超级管理员权限创建容器
--service-cluster-ip-range=10.0.0.0/24 \    // 启动service服务的虚拟网段
--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \    // 允许使用的插件
--authorization-mode=RBAC,Node \    // 授权模式
--enable-bootstrap-token-auth=true \    // 自动颁发证书
--token-auth-file=/opt/kubernetes/cfg/token.csv \   // 自动颁发证书用到的特点用户的信息
--service-node-port-range=30000-32767 \     // service端口范围
--kubelet-client-certificate=/opt/cfssl_file/k8s/server.pem \   // 访问kubelet使用的证书
--kubelet-client-key=/opt/cfssl_file/k8s/server-key.pem \
--tls-cert-file=/opt/cfssl_file/k8s/server.pem  \               // 访问apiservice使用的证书
--tls-private-key-file=/opt/cfssl_file/k8s/server-key.pem \
--client-ca-file=/opt/cfssl_file/k8s/ca.pem \
--service-account-key-file=/opt/cfssl_file/k8s/ca-key.pem \
--etcd-cafile=/opt/cfssl_file/etcd/ca.pem \                     // 访问 etcd 使用的证书
--etcd-certfile=/opt/cfssl_file/etcd/server.pem \
--etcd-keyfile=/opt/cfssl_file/etcd/server-key.pem \
--audit-log-maxage=30 \
--audit-log-maxbackup=3 \
--audit-log-maxsize=100 \
--audit-log-path=/opt/kubernetes/logs/k8s-audit.log"

$ systemctl daemon-reload
```

配置`kube-controller-manager`

```bash
$ vim /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-controller-manager.conf
ExecStart=/opt/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target

$ vim /opt/kubernetes/cfg/kube-controller-manager.conf
KUBE_CONTROLLER_MANAGER_OPTS="--logtostderr=false \
--v=2 \
--log-dir=/opt/kubernetes/logs \
--leader-elect=true \   // 多个apiservice 自动选一个调度
--master=127.0.0.1:8080 \ // 多个apiservice ip地址 因为在当前主机 可以写ip也可以写127
--address=127.0.0.1 \   \\ 监听的ip地址
--allocate-node-cidrs=true \    // 是不是支持网络插件
--cluster-cidr=10.244.0.0/16 \  // 基于网络插件分配的网络地址
--service-cluster-ip-range=10.0.0.0/24 \    // 客户端的地址范围
--cluster-signing-cert-file=/opt/cfssl_file/k8s/ca.pem \
--cluster-signing-key-file=/opt/cfssl_file/k8s/ca-key.pem  \
--root-ca-file=/opt/cfssl_file/k8s/ca.pem \
--service-account-private-key-file=/opt/cfssl_file/k8s/ca-key.pem \
--experimental-cluster-signing-duration=87600h0m0s"
```

配置`kube-controller-manager`

```bash
$ vim cfg/kube-scheduler.conf
KUBE_SCHEDULER_OPTS="--logtostderr=false \
--v=2 \
--log-dir=/opt/kubernetes/logs \
--leader-elect \
--master=127.0.0.1:8080 \
--address=127.0.0.1"

$ vim /usr/lib/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-scheduler.conf
ExecStart=/opt/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target

$ systemctl daemon-reload
```

#### 4. 启动master

```bash
$ systemctl start kube-apiserver.service
$ systemctl enable kube-apiserver.service

$ systemctl start kube-scheduler.service
$ systemctl enable kube-scheduler.service

$ systemctl start kube-controller-manager.service
$ systemctl enable kube-controller-manager.service

# 查看服务是否正常运行
$ ps aux | grep kube
```

#### 5. master 自动颁发证书

`kube-apiserver.conf` 配置文件的 `enable-bootstrap-token-auth` 和 `token-auth-file` 要配置正确

配置授权文件:

```
$ vim /opt/kubernetes/cfg/token.csv
c47ffb939f5ca36231d9e3121a252940,kubelet-bootstrap,10001,"system:node-bootstrapper"
```

文本内容用`,`号隔开分别代表:秘钥,用户名,过期时间,命名空间

给`kubelet-bootstrap`授权:
```bash
$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
```

#### 6. 部署node节点

各节点不同点：

名称各节点不同，是当前主机的`hostname`，和 `ip` 地址：

1. `kubelet.kubelet.conf.hostname-override`
2. `kubelet.bootstrap.kubeconfig.server` 都为主节点ip
3. `kube-proxy.kube-proxy-config.yml.hostnameOverride`
4. `kube-proxy.kube-proxy.kubeconfig.server`都为主节点ip

配置 `kubelet`：

```bash
$ vim cfg/kubelet-config.yml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local 
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /opt/cfssl_file/k8s/ca.pem 
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
maxOpenFiles: 1000000
maxPods: 110

$ vim cfg/kubelet.conf
KUBELET_OPTS="--logtostderr=false \
--v=2 \
--log-dir=/opt/kubernetes/logs \
--hostname-override=node01 \            // 主机名
--network-plugin=cni \
--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \
--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \
--config=/opt/kubernetes/cfg/kubelet-config.yml \
--cert-dir=/opt/cfssl_file/k8s \
--pod-infra-container-image=lizhenliang/pause-amd64:3.0"



$ vim cfg/bootstrap.kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /opt/cfssl_file/k8s/ca.pem
    server: https://192.168.205.120:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubelet-bootstrap
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: kubelet-bootstrap
  user:
    token: c47ffb939f5ca36231d9e3121a252940


$ vim /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
After=docker.service
Before=docker.service

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kubelet.conf
ExecStart=/opt/kubernetes/bin/kubelet $KUBELET_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```


配置 `kube-proxy`：

```bash
$ vim cfg/kube-proxy.kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /opt/cfssl_file/k8s/ca.pem
    server: https://192.168.205.120:6443    // master地址
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kube-proxy
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: kube-proxy
  user:
    client-certificate: /opt/cfssl_file/k8s/kube-proxy.pem
    client-key: /opt/cfssl_file/k8s/kube-proxy-key.pem


$ vim cfg/kube-proxy.conf
KUBE_PROXY_OPTS="--logtostderr=false \
--v=2 \
--log-dir=/opt/kubernetes/logs \
--config=/opt/kubernetes/cfg/kube-proxy-config.yml"

$ vim cfg/kube-proxy-config.yml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
address: 0.0.0.0
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig
hostnameOverride: node01 // 当前node的主机名
clusterCIDR: 10.0.0.0/24
mode: ipvs
ipvs:
  scheduler: "rr"
iptables:
  masqueradeAll: true

# 服务脚本
$ vim /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-proxy.conf
ExecStart=/opt/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

# 刷新配置
$ systemctl daemon-reload
```

启动服务
```bash
$ systemctl daemon-reload
$ systemctl start kube-proxy.service
$ systemctl start kubelet.service
$ systemctl enable kube-proxy.service
$ systemctl enable kubelet.service

# 查看日志
$ tail -f /opt/kubernetes/logs/kubelet.INFO
- No valid private key and/or certificate found, reusing existing private key or creating a new one
# 表示kubelet没有证书

# 查看正在请求证书worker
$ kubectl get csr
NAME                                                   AGE     REQUESTOR           CONDITION
node-csr-0W-KQ3YNy19wWMcNxSioocSIz8KEugRd0sfuqXXYeP0   3m53s   kubelet-bootstrap   Pending
node-csr-gorpJneD7gOGGBLJ0hGj6Z0dTTfAjCOyZhtbMWCk5LU   10m     kubelet-bootstrap   Pending
# 因为启动了两个worker node，所以有两条

# 办法证书
$ kubectl certificate approve node-csr-0W-KQ3YNy19wWMcNxSioocSIz8KEugRd0sfuqXXYeP0
$ kubectl certificate approve node-csr-gorpJneD7gOGGBLJ0hGj6Z0dTTfAjCOyZhtbMWCk5LU

# 在查看状态已经改变
$ kubectl get csr

# 此时可以看到worker 状态为NotReady因为没有网络插件配置
$ kubectl get node
node01   NotReady   <none>   4m31s   v1.16.0
node02   NotReady   <none>   3m5s    v1.16.0
```


#### 7.安装网络插件

`kubelet.conf`内指定使用了`cni`网络插件

在`worker`节点操作：

```bash
# 自行下载cni插件 cni-plugins-linux-amd64-v0.8.2.tgz
# 创建目录
$ mkdir -pv /opt/cni/bin /etc/cni/net.d
# 解压
$ tar xvf cni-plugins-linux-amd64-v0.8.5.tgz -C /opt/cni/bin/ 
```

在`master`执行在`yaml`文件:

```bash
$ kubectl apply -f kube-flannel.yaml
```
新增`yaml`文件`$ vim kube-flannel.yaml`

```yaml
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: psp.flannel.unprivileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default
    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  volumes:
    - configMap
    - secret
    - emptyDir
    - hostPath
  allowedHostPaths:
    - pathPrefix: "/etc/cni/net.d"
    - pathPrefix: "/etc/kube-flannel"
    - pathPrefix: "/run/flannel"
  readOnlyRootFilesystem: false
  # Users and groups
  runAsUser:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  # Privilege Escalation
  allowPrivilegeEscalation: false
  defaultAllowPrivilegeEscalation: false
  # Capabilities
  allowedCapabilities: ['NET_ADMIN']
  defaultAddCapabilities: []
  requiredDropCapabilities: []
  # Host namespaces
  hostPID: false
  hostIPC: false
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  # SELinux
  seLinux:
    # SELinux is unsed in CaaSP
    rule: 'RunAsAny'
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
rules:
  - apiGroups: ['extensions']
    resources: ['podsecuritypolicies']
    verbs: ['use']
    resourceNames: ['psp.flannel.unprivileged']
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes/status
    verbs:
      - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "cniVersion": "0.2.0",
      "name": "cbr0",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds-amd64
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: beta.kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: beta.kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
      hostNetwork: true
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: lizhenliang/flannel:v0.11.0-amd64 
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: lizhenliang/flannel:v0.11.0-amd64 
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
             add: ["NET_ADMIN"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
        - name: run
          hostPath:
            path: /run/flannel
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: flannel-cfg
          configMap:
            name: kube-flannel-cfg

```

查看执行结果
```bash
$ kubectl get pods -n kube-system
NAME                          READY   STATUS     RESTARTS   AGE
kube-flannel-ds-amd64-8b8pk   1/1     Running    0          3m47s
kube-flannel-ds-amd64-kw8hp   0/1     Init:0/1   0          3m47s
# 上面显示已经成功一个

# 在查看节点状态
$ kubectl get nodes
node01   NotReady   <none>   113m   v1.16.0
node02   Ready      <none>   112m   v1.16.0
# node02已经是ready状态
```

授权`apiservice`可以访问`kubelet`，在`master`节点执行:

```bash
$ vim apiserver-to-kubelet-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
      - pods/log
    verbs:
      - "*"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes


# 执行
$ kubectl apply -f apiserver-to-kubelet-rbac.yaml
```

* 如果出现`service`代理只能在本地`pods`访问的情况，可能是网络`flannel`配置错误

```bash
# 查看各节点INTERNAL-IP是否为期望IP
$ kubectl get nodes -o wide

# 错误的话再修改kubelet配置文件
KUBELET_EXTRA_ARGS="--node-ip=[期望ip] ....
```

```bash
# 查看ip是否正确，有的虚拟机可能eth0网卡是用来ssh连接的10.0.2.15
$ kubectl describe node node02 | grep public-ip
flannel.alpha.coreos.com/public-ip: 192.168.205.121

# 错误处理 指定flannel启动的网卡
$ kube-flannel.yml
...
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.11.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1  // 指定为eth1网卡
...
```

> 这个问题再使用 `kubeadm` 安装的时候也说一样的
> 
> kubeadm init --pod-network-cidr 10.244.0.0/16 --apiserver-advertise-address [指定eth1IP] 


##### 8. 安装管理界面

> https://www.kuboard.cn/

比官方的好用

##### 9. 安装k8s内部DNS解析
```bash
$ vim coredns.yaml
```

```yaml
# Warning: This is a file generated from the base underscore template file: coredns.yaml.base

apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
  labels:
      addonmanager.kubernetes.io/mode: EnsureExists
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            upstream
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  # replicas: not specified here:
  # 1. In order to make Addon Manager do not reconcile this replicas parameter.
  # 2. Default is 1.
  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      serviceAccountName: coredns
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      containers:
      - name: coredns
        image: lizhenliang/coredns:1.2.2
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.0.0.2 
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP

```

```shell script
$ kubectl apply -f coredns.yaml
# 查看运行结果
$ kubectl get pods -n kube-system
```